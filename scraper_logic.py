from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
from bleach import clean
import time

def wait_for_full_description(driver, selector, min_length=500, timeout=15):
    end_time = time.time() + timeout
    while time.time() < end_time:
        try:
            elem = driver.find_element(By.CSS_SELECTOR, selector)
            if len(elem.text.strip()) >= min_length:
                return True
        except Exception:
            pass
        time.sleep(0.5)
    return False

def extract_job_details(driver, url):
    driver.get(url)
    print(f"\n Visiting: {url}")

    try:
        title = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "h1.font-heading-3"))
        ).text.strip()
    except TimeoutException:
        title = "[Not Found]"

    try:
        location = driver.find_element(By.CSS_SELECTOR, "span.loc").text.strip()
    except Exception:
        location = "[Not Found]"

    
    try:
        try:
            company_elem = driver.find_element(By.CSS_SELECTOR, "a.companyInfo")
        except:
            company_elem = driver.find_element(By.CSS_SELECTOR, "span.companyInfo")
        company = company_elem.text.strip()
    except Exception:
        company = "[Not Found]"

   
    if wait_for_full_description(driver, "div.inner-content", min_length=500):
        soup = BeautifulSoup(driver.page_source, "html.parser")
        desc_container = soup.select_one("div.inner-content")
        with open("job_debug_raw.html", "w", encoding="utf-8") as f:
            f.write(desc_container.prettify() if desc_container else "")
        raw_html = desc_container.decode_contents() if desc_container else "[Not Found]"
    else:
        raw_html = "[Not Found or Incomplete]"

    allowed_tags = ['p', 'br', 'ul', 'li', 'ol', 'strong', 'em', 'h2', 'h3', 'a', 'b']
    allowed_attrs = {'a': ['href', 'title']}
    if "<span" in raw_html:
        raw_html = raw_html.replace("<span>", "").replace("</span>", "")
    description = clean(raw_html, tags=allowed_tags, attributes=allowed_attrs)

    print("======== JOB DEBUG INFO ========")
    print("üîó URL:", url)
    print("\nüî§ RAW HTML EXCERPT:\n", raw_html[:1000])
    print("\nüßº CLEANED HTML EXCERPT:\n", description[:1000])
    print("=================================\n")

    print("===== JOB DETAIL EXTRACTED =====")
    print("Title:", title)
    print("Location:", location)
    print("Link:", url)
    print("\nDescription Preview:\n", description[:400], "\n")

    return {
        "title": title,
        "company": company,
        "location": location,
        "link": url,
        "description": description
    }


def scrape_jobs(title, location, max_jobs=10, seniority=None):
    print("üîç Starting real job scraping with anti-detection...")
    
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox") 
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    
    # === PROVEN ANTI-DETECTION ===
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36"
    options.add_argument(f"--user-agent={user_agent}")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument("--disable-web-security")
    options.add_argument("--allow-running-insecure-content")
    
    driver = webdriver.Chrome(options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    try:
        # Load efinancialcareers.com
        print("üåê Loading efinancialcareers.com...")
        driver.get("https://www.efinancialcareers.com/")
        time.sleep(3)
        
        # Find and fill search fields
        print("üîç Looking for search fields...")
        title_input = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "input[placeholder*='Job title'], input[name*='title'], input[id*='search']"))
        )
        location_input = driver.find_element(By.CSS_SELECTOR, "input[placeholder*='Location'], input[name*='location']")
        
        print("‚å®Ô∏è Filling search form...")
        title_input.clear()
        title_input.send_keys(title)
        location_input.clear() 
        location_input.send_keys(location)
        
        # Submit search
        print("üîç Submitting search...")
        submit_button = driver.find_element(By.CSS_SELECTOR, "button[type='submit'], input[type='submit']")
        submit_button.click()
        time.sleep(5)
        
        # Extract job results (simplified for testing)
        print("üìã Extracting job results...")
        job_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='/job/']")[:max_jobs]
        
        results = []
        for i, link in enumerate(job_links[:3]):  # Test with first 3 jobs
            results.append({
                "title": f"Job {i+1}",
                "company": "Test Company",
                "location": location,
                "link": link.get_attribute("href") or "#",
                "description": "Real job scraping is working!"
            })
        
        driver.quit()
        return results if results else [{"title": "Search Completed", "company": "Success", "location": location, "link": "#", "description": f"Found {len(job_links)} job links total"}]
        
    except Exception as e:
        print(f"‚ùå Scraping error: {str(e)}")
        try:
            driver.quit()
        except:
            pass
        return [{"title": "Scraping Error", "company": "Error", "location": location, "link": "#", "description": f"Error during scraping: {str(e)}"}]
